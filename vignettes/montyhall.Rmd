---
title: 'Simulating and breaking the Monty Hall Problem'
author: "Sean Maden"
date: "2/3/2020"
output: html_document
---

```{r setup, include = FALSE, eval = TRUE}
# require(devtools); install_github("metamaden/montyhall")
# library(montyhall)
knitr::opts_chunk$set(eval = TRUE, echo = TRUE)
```

```{r func, echo = FALSE}
library(ggplot2)
library(gridExtra)
library(ggridges)

mhsim <- function(niter = 100, seed = 1, ndoors = 3, 
                  ndec1 = 1, ndec2 = 1, nprizes = 1,
                  nrevealdif = 1, prize.index = NULL,
                  selectdec1 = "random", doorswitch = 1, 
                  montyselect = "random", verbose.results = FALSE){
  set.seed(seed)
  lr <- c() # new return object
  if(verbose.results){
    lv <- list()
  }
  doorseq <- seq(1, ndoors, 1)
  for(i in 1:niter){
    # pick prize door index
    which.prize <- sample(doorseq, nprizes)
    # run decision 1
    if(selectdec1 == "random"){
      dec1select <- sample(doorseq, ndec1)
    } else{
      if(is.numeric(selectdec1) & selectdec1 %in% doorseq){
        dec1select <- selectdec1
      } else{
        stop("Invalid decision 1 selectoin specified.")
      }
    }
    # run montyselect
    doorremain1 <- doorseq[!doorseq == dec1select] # exclude player first selection
    nr <- length(doorremain1) - nrevealdif # calculate the reveal difference
    # validate reveal difference value
    if(nr < 0 | nr > length(doorremain1) - 1){
      stop("Too many doors specified for Monty to reveal. Increase `nrevealdif`.")
    }
    if(montyselect == "random"){
      # if more than 1 prize, allow monty to reveal n - 1 prizes
      if(length(which.prize) > 1){
        mdooroptions <- doorremain1
      } else{
        mdooroptions <- doorremain1[!doorremain1 %in% which.prize]
      }
      if(length(mdooroptions) < 2){
        mselect <- mdooroptions
      } else{
        mselect <- sample(mdooroptions, nr)
      }
    }
    # run decision 2
    # exclude monty's doors and decision 1 doors from switch options
    doorremain2 <- doorseq[!doorseq %in% c(mselect, dec1select)]
    # parse switch likelihood
    if(is.numeric(doorswitch) & doorswitch >= 0 & doorswitch <= 1){
      ssvar <- ifelse(doorswitch == 1, "switch", 
             sample(c(rep("switch", 100*doorswitch), 
                      rep("stay", 100 - 100*doorswitch)), 1))
    } else{
      stop("Invalid doorswitch value.")
    }
    # evalue switch decision
    if(ssvar == "switch"){
      if(length(doorremain2) > 1){
        dec2select <- sample(doorremain2, ndec2)
      } else{
        dec2select <- doorremain2
      }
    } else{
      dec2select <- dec1select
    }
    # evaluate results
    lr <- c(lr, ifelse(dec2select %in% c(which.prize), "win", "loss"))
    if(verbose.results){
      lv[[length(lv) + 1]] <- list("which.prize" = which.prize,
                                   "dec1select" = dec1select,
                                   "nr" = nr,
                                   "montyselect" = mselect,
                                   "dec2select" = dec2select)
    }
  }
  if(verbose.results){
    return(list("results" = lr, "details" = lv))
  } else{
    return(lr) # return iterations results
  }
}

getfw = function(nsimulations = 5, niterations = 2, ndoors = 3, 
                 prize.index = NULL, doorswitch = 1,
                 verbose.results = FALSE){
  fw <- c() # vector of win fractions across simulations
  vr <- list() # results for verbose results option
  for(s in 1:nsimulations){
    lrs <- mhsim(seed = s, niter = niterations, ndoors = ndoors,
                 prize.index = prize.index, doorswitch = doorswitch,
                 verbose.results = verbose.results)
    if(verbose.results){
      vr[[paste0(s)]] <- lrs
    } else{
      fw <- c(fw, length(which(lrs == "win"))/length(lrs))
    }
  }
  if(verbose.results){
    return(vr)
  } else{
    return(fw)
  }
}

getggdat <- function(ld){
  dfp <- matrix(nrow = 0, ncol = 2)
  for(i in 1:length(lnd)){
    nrep <- length(lnd[[i]])
    ndi <- as.numeric(names(lnd)[i])
    ndv <- rep(as.numeric(ndi), nrep)
    dati <- lnd[[i]]
    lmi <- matrix(c(dati, ndv), ncol = 2, byrow = FALSE)
    dfp <- rbind(dfp, lmi)
  }
  dfp <- as.data.frame(dfp, stringsAsFactors = F)
  colnames(dfp) <- c("fract.win", "ndoors")
  ulvl <- unique(dfp[,2])
  olvl <- ulvl[order(unique(dfp[,2]))]
  dfp[,2] <- factor(dfp[,2], levels = olvl)
  return(dfp)
}

getlinedat <- function(ld, ribbontype = "sd", xtitle = "ndoors"){
  dfp <- matrix(nrow = 0, ncol = 4)
  for(i in 1:length(lnd)){
    dati <- lnd[[i]]
    ndi <- as.numeric(names(lnd)[i])
    meandat <- mean(dati)
    # parse ribbon overlay
    if(ribbontype == "sd"){
      mindat <- meandat - sd(dati)
      maxdat <- meandat + sd(dati)
    }
    if(ribbontype == "minmax"){
      mindat <- min(dati)
      maxdat <- max(dati)
    }
    lmi <- matrix(c(meandat, maxdat, mindat, ndi), nrow = 1)
    dfp <- rbind(dfp, lmi)
  }
  dfp <- as.data.frame(dfp, stringsAsFactors = F)
  colnames(dfp) <- c("fract.win", "max", "min", xtitle)
  return(dfp)
}

getlineplot <- function(ld, ptitle = "Plot title", ribbontype = "sd",
                        xlim = NULL, ylim = NULL, xlab = "ndoors"){
  dfp <- getlinedat(ld, ribbontype = ribbontype)
  if(is.null(xlim) & is.null(ylim)){
    plp <- ggplot(dfp, aes(ndoors)) +
      geom_line(aes(y = fract.win), colour = "blue") + 
      geom_ribbon(aes(ymin = min, ymax = max), alpha = 0.2) +
      theme_bw() + ggtitle(ptitle) + xlab(xlab)
  }
  if(!is.null(xlim) & !is.null(ylim)){
    plp <- ggplot(dfp, aes(ndoors)) +
      geom_line(aes(y = fract.win), colour = "blue") + 
      geom_ribbon(aes(ymin = min, ymax = max), alpha = 0.2) +
      theme_bw() + ggtitle(ptitle) +
      ylim(ylim[1], ylim[2]) +
      xlim(xlim[1], xlim[2]) + xlab(xlab)
  }
  return(plp)
}

getprettyplots <- function(ld, topmain = "Top Title"){
  # fromat results data
  dfp <- getggdat(ld)
  
  # violin plots
  p1 <- ggplot(dfp, aes(x = ndoors, y = fract.win, fill = ndoors)) +
    geom_violin() + theme_bw() + theme(legend.position = "none") +
    ggtitle("Violin plots")
  
  # ridge plots
  p2 <- ggplot(dfp, aes(x = fract.win, y = ndoors, fill = ndoors)) +
    geom_density_ridges() + theme_bw() + theme(legend.position = "none") +
    ggtitle("Ridge plots")
  
  # line plot
  p3 <- getlineplot(ld, ptitle = "Line plot")
  
  # make the composite plot
  grid.arrange(p1, p2, p3, ncol = 3, top = topmain)
}


```

On a gameshow stage before you wait three closed doors, behind which have been deposited 2 goats and one prize, respectively. You are called on to pick a door to open and reveal either a goat or a prize. Monty Hall, the gameshow's host, proceeds to reveal a goat behind one of the two unpicked doors. You must then decide whether to stick with your original choice or switch to the final unpicked door. What do you do?

This is the [Monty Hall Problem](https://en.wikipedia.org/wiki/Monty_Hall_problem), a kind of logic puzzle involving conditional probability. Assuming you value prizes over goats, it can be readily shown that you should *always* switch whichever door you've initially picked to maximize your success probability. If you stick with your first choice, your success probability never exceeds 1 out of 3 tries, while switching increases your probability to 2 out of 3, a pretty substantial increase!

It's telling that the Monty Hall Problem still serves as a good brain teaser to this day. Given its simple rules and decision parameters, it's a problem that lends itself to programmatic simulation. I wrote a script to allow exploration of this problem. The full code with plotting utilities is available at the project's GitHub repo. We can use this simulation to illustrate some features and ways of thinking about the problem, including better ways of intuiting the answer should we forget!

Formulating the game

We can call the typical formulation of the game, as above, the "canonical" or "classical" formulation. This has the following attributes or rules:

1. Three doors total, behind which 1 has a prize, and the remaining 2 have goats.
2. The player picks a door.
3. Monty reveals one of the two remaining doors to be a goat.
4. The player decides whether to stick with their initial choice (step 2) or switch to the last unpicked door.
5. The final player-selected door is revealed to be either a goat or the prize.

When we plan how to execute a simulation in code, pseudocode can be a helpful tool. This is an abstraction of the task that is programming language- and code-agnostic. The pseudocode to carry out the classical problem would look something like:

* function do Monty_Hall_Simulation:
  + get door_indices from 1:ndoors
  + assign prize_door
  + randomize player_door_index1
  + get remaining_doors
  + get monty_door_indices up to ndoors - 2
  + get player_door_index2 as remaining_door_index
  + if player_door_index2 == prize_door, return "win", else "lose"

Following the tasks from this pseudocode logic, I wrote the simulation function.

# The simulation function

The simulation function I wrote, `mhsim()`, runs iterations of the Monty Hall problem with various allowances for changing underlying parameters. By default, the code will run game simulations or "iterations" up to the value of `niter` using the classic problem parameters above.

The arguments `niter` and `seed` are important to consider even when running the simulation function with defaults otherwise. The `niter` argument specifies the number of game iterations to simulate, while setting `seed` specifies the value passed to `set.seed()`. Setting the seed allows for *exact* replication of run results with the same seed, an important component for operations implementing randomness. As already mentioned, the simulation assumes random player selection in step 2 and Monty selection in step 3, with other possibilities for randomness including the player door switch frequency in step 4. I implemented randomization with the `sample` R function.

Iterations are currently implemented in a `for` loop. This could potentially be improved with parallelization and [multithreading](https://en.wikipedia.org/wiki/Thread_(computing)#Multithreading) as with the [`pools`](https://cran.r-project.org/web/packages/pool/index.html) or [snow](https://cran.r-project.org/web/packages/snow/index.html) R packages, though there's a bit of a tradeoff. However convenient parallelization is made with specilized packages, this implies adding an extra dependency to the package and a bit more code. If the problem in question were considerably more memory taxing than the Monty Hall problem, this could justify parallelization. 

Importantly, parallelization breaks strict replicability because sub-tasks or jobs are returned in the order they finish. Practically, this means the ordering of game outcomes across iterations would change each time the function ran, even with the same `seed`. While this point is made somewhat moot since we're primarily interested in the win fraction, it could be a severe limitation in other settings.

To show how the simulation function achieves the tasks from the pseudocode, let's break down the steps of each iteration. First, the index of the prize door is specified.

```{r mh_setprize, eval = FALSE}
which.prize <- sample(doorseq, nprize)
```

Then the player's first decision is simulated.

```{r, mh_decision1, eval = FALSE}
dec1select <- sample(doorseq, ndec1)
```

Next, Monty reveals a goat. This entails either selecting 1 of 2 doors at random (e.g. if the player already picked the prize door, an unlikely event), or simply picking the last remaining door (e.g. if the player already picked a goat door, a likely event).

```{r mh_montypick, eval = FALSE}
mdooroptions <- doorremain1[!doorremain1 %in% which.prize]
if(length(mdooroptions) < 2){
  mselect <- mdooroptions
} else{
  mselect <- sample(mdooroptions, nr)
}
```

We check that the number of remaining "non-revealed" doors specified by `nrevealdif` is valid for the problem, then proceed to either specify a random set ofdoor indices to reveal or the only remaining valid door.

Next, the player either switches or stays on their initial door selection. Note how switch frequency from `doorswitch` impacts the likelihood of passing `switch` or `stay` to `ssvar`.

```{r mh_decision2, eval = FALSE}
if(ssvar == "switch"){
  if(length(doorremain2) > 1){
    dec2select <- sample(doorremain2, ndec2)
  } else{
    dec2select <- doorremain2
  }
}
```

The function then returns a vector of game outcomes (either `win` or `loss`) of length equal to `niter`. I've also included a `verbose.results` option that stores the granual game details for each iteration alongside outcome. I mainly included this for bug squashing.

Finally, note that I've wrapped the `mhsim` function into a new function, `getfw`. This is because `mhsim` runs a single simulation consisting of `niter` games, and we'd like to make it easy to run many simulations automatically. Thus `getfw` returns the win fraction per simulation across `nsimulations`, where certain arguments like `ndoors` can be passed to `mhsim` when we want to change simulation parameters.

# Simulating the canonical/classic problem

Let's study the impact of varying the number of simulations and iterations per simulation on the distribution of win frequencies across simulations. We'll start small with just 5 simulations of 2 games, and increase this to 100 and then 1,000 simulations and iterations, respectively. We can use the wrapper function `getfw` to generate and store the reproducible simulation data.

Let's generate the simulation data with the 3 parameter sets, and time it. Note we'll use a `for` loop to iterate over vectors of 3 values for simulations and iterations, using the given index to point to the parameters for each run.

```{r mh_classic_3runs}
# parameter sets
simv <- c(5, 100, 1000)
iterv <- c(2, 100, 1000)
lr <- list()
t1 <- Sys.time()
for(s in 1:length(simv)){
  runname <- paste0(simv[s], ";", iterv[s])
  lr[[runname]] <- getfw(nsimulations = simv[s], niterations = iterv[s])
}
tdif <- Sys.time() - t1
```

The 3 runs completed in `r round(tdif, 0)` seconds. With so few iterations and simulations in the first run, there's huge variance in the win fraction (standard deviation of `r round(sd(lr[[1]]), 2)`). Increasing iterations and simulations each to 100 already shows the distribution converging on the expected win frequency of 0.66. Further increase to 1,000 simulations and iterations results in a more clearly normal distribution with much tighter standard deviation of `r round(sd(lr[[3]], 2)`.

Let's now show the composite plot of win frequency distributions across the 3 runs. Note I've stored the run info (number of simulations and iterations per run) in the list names, and we can unpack these with regular exressions using `gsub()` for the respective plot titles. We'll use `par` to manage the plot output and formatting, where `nrow = c(1, 3)` specifies the plot output conforms to a matrix of 1 row and 3 columns, and `oma = c(3, 3, 3, 1)` adds outer margin whitespace for axis labels. We'll remove redundant axis labels for each plot and add these back with `mtext()`.

```{r mh_classic_3hist}
pdf("mh_3runs.pdf", 10, 4)
# format image output
par(mfrow = c(1, 3), oma = c(3, 3, 3, 1))
for(r in 1:length(lr)){
  rdat <- lr[[r]]
  # get plot title info
  rname <- names(lr)[r]
  simr <- gsub(";.*", "", rname)
  iterr <- gsub(".*;", "", rname)
  pmain <- paste0(simr," simulations\n", iterr, " iterations")
  # add run histogram to image output
  hist(rdat, main = pmain, xlab = "", ylab = "")
}
# add outer axis labels
mtext("Win Frequency", side = 1, outer = T)
mtext("Number of Simulations", side = 2, outer = T)
dev.off()
```

If you prefer to be more precise about the increase in normalcy, we can show greater distribution normalcy by high confidence from the 
[Shapiro-Wilk Normality test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test)
with `shapiro.test`, where we test the null hypothesis that data were drawn from a normal distribution.

```{r st_mhbasic}
# run normalcy tests
st1 <- shapiro.test(lr[["5;2"]])$p.value
st2 <- shapiro.test(lr[["100;100"]])$p.value
st3 <- shapiro.test(lr[["1000;1000"]])$p.value
```

With increased simulations and iterations, our p-value increase from `r round(st1, 3)` in the first and smallest run to `r round(st3, 3)` in the third and largest run. Practically, this means confidence to reject the alternative hypothesis (e.g. of non-normality) is decreasing as the underlying simulation win fractions converge on an approximately normal distribution.

# Bending the rules

I've written `mhsim()` and its `getfw()` wrapper to allow us to modify the game rules in a few ways. Exploring how bending the rules impacts simulated win frequencies can lead us to better understand the roles of key game conditions. Showing quantitatively how win frequencies change in light of different conditions can help us to better characterize the problem and inform the notion that it's *always* better to switch doors under the classical game rules.

The first condition we'll explore is the number of doors. I've allowed for the door quantity to be changed with the `ndoors` argument. Practically, this just changes the game setup for an interation by defining a vector of sequential door indices of length `ndoors`. Thus increasing `ndoors` from 3 still preserves other parameters for the classical game be default. 

I've also allowed for changing the frequency with which the player switches doors with `doorswitch`. The default value of 1 means the player switches 100% of the time, and setting this a lower value between 0 and 1 means decreasing the switch frequency. I did this by implementing `sample()` to randomly select from what's essentiallt a weighted binomial distribution (e.g. possible outcomes are binary but each outcome has a distinct weight). If `doorswitch = 0.2`, we parse player decision by sampling from a distribution where 20% of options are "switch" and (100 - 20 = ) 80% of options are "stay".

# Many doors, and an extrapolation mnemonic

One of the more useful [mnemonic devices](https://en.wikipedia.org/wiki/Mnemonic) I've encountered for intuiting the answer to the Monty Hall Problem is to increase the number of doors. Maybe we're unsure if switching doors will increase our odds when there are just 3 doors. But if there are 100 doors, and Monty reveals goats behind 98 of them, it's much clearer that switching will increase our chances of winning. We can quantitatively visualize this intuitively useful device with the simulation code. 

Let's now generate and time the results from running 100 simulations of 100 iterations each, varying `ndoors` from 3 to 100 with otherwise classical rule parameters.

```{r mh_run_ndoors-sim}
# get win frequencies from varying ndoors
simi = 100; iteri = 100
ndoorl <- seq(3, 103, 10)
seedl <- seq(1, 100, 1)
lnd <- list()
t1 <- Sys.time()
for(nd in ndoorl){
  fw <- getfw(simi, iteri, nd)
  lnd[[paste0(nd)]] <- fw
}
tdif <- Sys.time() - t1
# store the reference plot
pref <- getlineplot(lnd, ptitle = "Canonical rules, varying doors")
```

All runs completed in `r round(tdif, 0)` seconds. Let's visualize results in a few different ways. First, we'll generate [violin plots](https://en.wikipedia.org/wiki/Violin_plot), a powerful way of visualizing data in a relatively distribution-agnostic manner (and thus typically better than boxplots). Next, we'll use overlapping density plots, variously called "ridge plots" or "joyplots" after their use in the iconic visualization of CP 1919 pulsar's radio waves on the cover of Joy Division's Unknown Pleasures record ([awesome!](https://en.wikipedia.org/wiki/Unknown_Pleasures#Artwork_and_packaging)). Finally, we'll show line plots of run means with confidence boundaries. While the first two plots show the exact data distributions, we'll focus on the third line plots for their economy of space and meaningful reflection of simulation distribution properties.

To generate these visualizations, I've wrapped the code inot the several plotting utilities functions `getggdat()` (format data for violin and ridge plots), `getlinedat` (format data for line plots), `getlineplot()` (generates line plot), `getprettyplots()` (generate composite of 3 ggplot2 plot types). I won't exhaustively describe these, but will note the code may be generally useful if you're looking for a generalizable way of plotting data for your own data science project. This code makes use of the supremely awesome R packages [`ggplot2`](https://cran.r-project.org/web/packages/ggplot2/index.html) (powerful plotting functions and meta syntax), [`gridExtra`](https://cran.r-project.org/web/packages/gridExtra/index.html) (ridge plot options),
[`ggridges`](https://cran.r-project.org/web/packages/ggridges/index.html) (managing plot outputs and composite plotting).

```{r mh_doorvary_plots}
pdf("mh_ndoors_3plots.pdf", 10, 4)
getprettyplots(lnd, "Varying door count")
dev.off()
```

This quantitatively shows the magnitude of win likelihood increase with `ndoors` increase, reinforcing our intuition about the mnemonic device. It's also interesting to note how the standard deviation converges after the means in runs with higher door counts as the win frequency increase becomes both higher and more certain.

We'll focus on the line plot visualizations below, but I've allowed for two plot types with the `ribbontype` argument. This defines the gray-colored confidence visualization to be either the standard deviation of run distribution (if `sd`, the default), or the minimum and maximum win frequencies observed (if `minmax`). Let's show these side-by-side to illustrate the difference.

```{r mh_classic_lineplotcompare}
pclassic1 <- getlineplot(lnd, ptitle = "Std. dev. overlay", ribbontype = "sd")
pclassic2 <- getlineplot(lnd, ptitle = "Min. max. overlay", ribbontype = "minmax")

pdf("mh_2lineplots.pdf", 5, 3)
grid.arrange(pclassic1, pclassic2, top = "Ribbon overlay comparison", ncol = 2)
dev.off()
```

We'll lean on these line plot representations using distribution standard deviations to calculate the overlaid ribbons.

# What if the player doesn't always switch?

Let's now observe the impact of player switch frequency, or how often the player switches from their initial door selection. As mentioned, this is set by passing the decimal switch frequency to the `doorswitch` argument, which then parses player choice for each iteration from a weighted binomial distribution.

Let's run 10 simulations varying the switch frequency from 0% to 100% in increments of 10%. I'll store the results plots in the `plist` object, then make a composite plot of the 10 results plots. Note I've also set the x- and y-axis min and max values to be the same in `getlineplot` so that visual comparison is easier.

```{r mh_switchfreq_sim}
# get fwin dist across ndoors
plist <- list()
sfreq <- seq(0, 1, 0.1)
for(s in sfreq){
  simi = 100; iteri = 100
  ndoorl <- seq(3, 103, 10)
  seedl <- seq(1, 100, 1)
  lnd <- list()
  for(nd in ndoorl){
    fw <- getfw(simi, iteri, nd, doorswitch = s)
    lnd[[paste0(nd)]] <- fw
  }
  plist[[paste0(s)]] <- getlineplot(lnd, ptitle = paste0("S.F. = ", s),
                                    xlim = c(0, 100), ylim = c(0, 1))
  message(s)
}
```

The composite plot is then generated from the `plist` plots list as follows.

```{r mh_switchfreq_10sim}
grid.arrange(plist[[1]], plist[[2]], plist[[3]],
             plist[[4]], plist[[5]], plist[[6]],
             plist[[7]], plist[[8]], plist[[9]],
             plist[[10]],
             ncol = 5)
```

Across run sets of each door switch frequency, there's a clear transition from an approximate negative power function (e.g. x ^ -1, top leftmost plot), to something approaching a fractional power function (e.g. x ^ 1/2, bottom rightmost plot). 

Increasing run switch frequency incrementally under classical rules should show progressive increase in win fraction distributions. Let's generate and visualize the simulation results for this. Note, I'll appropriate my `getlineplot()` function for this as written, but in some applications it can be better to add code that explicitly handles specific axis variables like `ndoors` and `doorswitch`. The resulting plot shows a clear linear win fraction increase with switch frequency, maxing out at the now-familiar 2/3rds fraction.

```{r mh_switchfreq_classic_sim}
sfreq <- seq(0, 1, 0.1)
lnd <- list()
for(s in sfreq){
  simi = 100; iteri = 100
  seedl <- seq(1, 100, 1)
  fw <- getfw(simi, iteri, doorswitch = s)
  lnd[[paste0(s)]] <- fw
}
pdf("mh_switchfreq_classicrules.pdf", 4, 4)
getlineplot(lnd, ptitle = "Win Freq. by Switch Freq.", 
            xlim = c(0, 1), ylim = c(0, 1), 
            xlab = "Switch frequency")
dev.off()
```

# Conclusions and analysis extensions

We've explored simulations of the Monty Hall Problem using a brute force approach. By exploring changes in win frequency across varying problem conditions, we've proven that always switching doors will increase player win frequency. We've also quantitatively shown how improved win frequency converges as the number of doors is increased. Finally, we explored the role of certain conditions to the problem itself. Unsuprisingly, as player switch frequency increases, so too does win frequency. In so doing, we showed how switch frequency increase leads to different win frequency improvements across doors. 

This brute force simulation approach is one of many possible ways of sloving and exploring the Monty Hall Problem, and alternate approaches implementing Bayesian models could lead to further interesting insights. There are several other game conditions that could also be explored. These include changing the total number of doors with prizes, for games of at least 4 doors. Ultimately, I hope this investigation provided some useful code that equips you with a framework for investigating new problems through simulation.
